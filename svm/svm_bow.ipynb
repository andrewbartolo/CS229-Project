{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################################################################################\n",
    "#    Code for SVM Baseline using Bag-Of-Word Features and scikit sparsification               #\n",
    "#       functions (countVectorizer) for Sentiment Analysis on IMDB dataset                    #\n",
    "###############################################################################################\n",
    "\n",
    "#Import Libraries\n",
    "\n",
    "#Import Matrix Handling Capabilities\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "#Import File Handling\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "import re\n",
    "#Import scikit library features for model manipulation\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.kernel_approximation import RBFSampler\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.externals import joblib\n",
    "\n",
    "#Add path to files containing movie review train and test datasets \n",
    "pwd='/Users/pulkit/Google Drive/Stan Courses/4. Fall 2017/CS229 ML/Project/CS229-Project/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading word list!\n",
      "Loaded the word list!\n",
      "400000\n",
      "Going through Positive Files for Training Bag-Of-Words\n",
      "Positive files finished\n",
      "Going through Negative Files for Training Bag-Of-Words\n",
      "Negative files finished\n",
      "Going through Positive Files for Testing Bag-Of-Words\n",
      "Positive files finished\n",
      "Going through Negative Files for Testing Bag-Of-Words\n",
      "Negative files finished\n",
      "(50010, 166935)\n",
      "(100000, 166935)\n"
     ]
    }
   ],
   "source": [
    "###############################################################################################\n",
    "#       Extract Features for Train, Test and Adversary Cases from the raw files               #\n",
    "###############################################################################################\n",
    "\n",
    "\n",
    "##########################\n",
    "#  Import Training Data  #\n",
    "##########################\n",
    "\n",
    "# Add datapath to training dataset\n",
    "datapath=pwd+'stanford_train/'\n",
    "\n",
    "#Load Precomputed Wordlist\n",
    "print('Loading word list!')\n",
    "wordsList = np.load(datapath+'wordsList-lexic-sorted.npy').tolist()\n",
    "print('Loaded the word list!')\n",
    "\n",
    "numDictionaryWords=len(wordsList)\n",
    "print(numDictionaryWords)\n",
    "\n",
    "#Add Path to positive and negative review files\n",
    "positiveFiles = [datapath+'pos/' + f for f in listdir(datapath+'pos/') if isfile(join(datapath+'pos/', f))]\n",
    "negativeFiles = [datapath+'neg/' + f for f in listdir(datapath+'neg/') if isfile(join(datapath+'neg/', f))]\n",
    "numWords = []\n",
    "\n",
    "#define for string cleaning\n",
    "strip_special_chars = re.compile(\"[^A-Za-z0-9 ]+\")\n",
    "def cleanSentences(string):\n",
    "    '''\n",
    "    Cleans Sentences\n",
    "    '''\n",
    "    string = string.lower().replace(\"<br />\", \" \")\n",
    "    return re.sub(strip_special_chars, \"\", string.lower())\n",
    "\n",
    "#define for finding index in wordvec\n",
    "def findIndex(search_list, begin, end, key):\n",
    "    '''\n",
    "    Find Index of word in a list\n",
    "    '''\n",
    "    mid = int((end - begin + 1)/2) + begin\n",
    "    if end == begin:\n",
    "        if search_list[mid] == key:\n",
    "            return mid\n",
    "        else:\n",
    "            return -1\n",
    "    if end == begin + 1:\n",
    "        if search_list[begin] == key:\n",
    "            return begin\n",
    "        if search_list[end] == key:\n",
    "            return end\n",
    "        else:\n",
    "            return -1\n",
    "    if search_list[mid] < key:\n",
    "        return findIndex(search_list, mid, end, key)\n",
    "    return findIndex(search_list, begin, mid, key)\n",
    "\n",
    "# Adversaries for Positive and Negative Examples as Found by Naive Bayes\n",
    "negativeAdversaries=['edie','antwone','din','gunga','yokai']\n",
    "positiveAdversaries=['boll','410','uwe','tashan','hobgoblins']\n",
    "\n",
    "\n",
    "fileCounter = 0\n",
    "#Define a list for storing the reviews for normal dataset and adverserial dataset\n",
    "#Formed by replacing first word with an adversary word as found by NB\n",
    "#Corpus has size 50010(adversarialWords([0:10]),numTraining([10:25010]) and numTest ([25010:50010])Examples)\n",
    "#Corpus2 has size 10000(odd elements has the first word ina ctual sentence and even are numTraining+numTest Eg)\n",
    "corpus=[] \n",
    "corpus2=[]\n",
    "corpus.extend(negativeAdversaries) #So that sparsification leads same matrix size for prediction\n",
    "corpus.extend(positiveAdversaries)\n",
    "\n",
    "print('Going through Positive Files for Training Bag-Of-Words')\n",
    "for pf in positiveFiles:\n",
    "    with open(pf, \"r\") as f:\n",
    "        indexCounter = 0\n",
    "        line=f.readline()\n",
    "        cleanedLine = cleanSentences(line)\n",
    "        corpus.append(format(str(cleanedLine))) #Corpus contains original sentences and adversary words\n",
    "        \n",
    "        split = cleanedLine.split()\n",
    "        corpus2.append(split[0]) #Odd index of corpus2 contains the original first word\n",
    "        \n",
    "        adversaryIdx=np.random.randint(5, size=1,dtype='int32')\n",
    "        split[0]=positiveAdversaries[np.asscalar(adversaryIdx)]\n",
    "        cleanedLine=\" \".join(split)\n",
    "        corpus2.append(format(str(cleanedLine))) #Even index of corpus2 contains adversary sentence motivated by NB\n",
    "                \n",
    "        fileCounter = fileCounter + 1 \n",
    "print('Positive files finished')\n",
    "\n",
    "print('Going through Negative Files for Training Bag-Of-Words')\n",
    "for nf in negativeFiles:\n",
    "    with open(nf, \"r\") as f:\n",
    "        indexCounter = 0\n",
    "        line=f.readline()\n",
    "        cleanedLine = cleanSentences(line)\n",
    "        #split = cleanedLine.split()\n",
    "        corpus.append(format(str(cleanedLine)))\n",
    "        \n",
    "        split = cleanedLine.split()\n",
    "        corpus2.append(split[0])\n",
    "        \n",
    "        adversaryIdx=np.random.randint(5, size=1,dtype='int32')\n",
    "        split[0]=negativeAdversaries[np.asscalar(adversaryIdx)]\n",
    "        cleanedLine=\" \".join(split)\n",
    "        corpus2.append(format(str(cleanedLine)))\n",
    "        \n",
    "        fileCounter = fileCounter + 1 \n",
    "print('Negative files finished')\n",
    "\n",
    "#########################\n",
    "###Import Testing Data###\n",
    "#########################\n",
    "\n",
    "# Add datapath to training dataset\n",
    "\n",
    "datapath=pwd+'stanford_test/'\n",
    "\n",
    "positiveFiles = [datapath+'pos/' + f for f in listdir(datapath+'pos/') if isfile(join(datapath+'pos/', f))]\n",
    "negativeFiles = [datapath+'neg/' + f for f in listdir(datapath+'neg/') if isfile(join(datapath+'neg/', f))]\n",
    "\n",
    "fileCounter = 0\n",
    "print('Going through Positive Files for Testing Bag-Of-Words')\n",
    "for pf in positiveFiles:\n",
    "    with open(pf, \"r\") as f:\n",
    "        indexCounter = 0\n",
    "        line=f.readline()\n",
    "        cleanedLine = cleanSentences(line)\n",
    "        #split = cleanedLine.split()\n",
    "        corpus.append(format(str(cleanedLine)))\n",
    "        \n",
    "        split = cleanedLine.split()\n",
    "        corpus2.append(split[0])\n",
    "        \n",
    "        adversaryIdx=np.random.randint(5, size=1,dtype='int32')\n",
    "        split[0]=positiveAdversaries[np.asscalar(adversaryIdx)]\n",
    "        cleanedLine=\" \".join(split)\n",
    "        corpus2.append(format(str(cleanedLine)))\n",
    "        \n",
    "        fileCounter = fileCounter + 1 \n",
    "print('Positive files finished')\n",
    "\n",
    "print('Going through Negative Files for Testing Bag-Of-Words')\n",
    "for nf in negativeFiles:\n",
    "    with open(nf, \"r\") as f:\n",
    "        indexCounter = 0\n",
    "        line=f.readline()\n",
    "        cleanedLine = str(cleanSentences(line))\n",
    "        #split = cleanedLine.split()\n",
    "        corpus.append(format(cleanedLine))\n",
    "        \n",
    "        split = cleanedLine.split()\n",
    "        corpus2.append(split[0])\n",
    "        \n",
    "        adversaryIdx=np.random.randint(5, size=1,dtype='int32')\n",
    "        split[0]=negativeAdversaries[np.asscalar(adversaryIdx)]\n",
    "        cleanedLine=\" \".join(split)\n",
    "        corpus2.append(format(str(cleanedLine)))\n",
    "        \n",
    "        fileCounter = fileCounter + 1 \n",
    "print('Negative files finished')\n",
    "\n",
    "#Make a sparse Bag-oF-Word Matrix as Feature Set for both normal and adversary case\n",
    "vectorizer = CountVectorizer()\n",
    "X= vectorizer.fit_transform(corpus)\n",
    "X_adversary=vectorizer.fit_transform(corpus2)\n",
    "\n",
    "print(X.shape)\n",
    "print(X_adversary.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing Feature Set\n",
      "Feature Set Computed\n",
      "Fitting SVM\n",
      "Fitting Done\n",
      "Misclassified Fraciton on Train Dataset=0.0368\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['linear_with_bow.txt']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "###############################################################################################\n",
    "#                           Fit Models and check training error                               #\n",
    "#  Uses SGD with hinge loss and l2 normalization to fit SVM. Generate features appropriately  #\n",
    "###############################################################################################\n",
    "\n",
    "#Choose:\n",
    "#  Kernels from linear, polyD (polynomial with degree d), RBF\n",
    "#  L2 penalty al (start from 1e-4)\n",
    "#  File name for saving model\n",
    "kernel='linear' \n",
    "ALPHA=1e-4\n",
    "label=\"linear_with_bow.txt\"\n",
    "\n",
    "# Slice Training Data\n",
    "X_train=X[10:25010,:]\n",
    "\n",
    "#Generate Labels\n",
    "labels=-1*np.ones(numFiles)\n",
    "labels[0:int(numFiles/2)]=np.ones(int(numFiles/2))\n",
    "y_train=labels\n",
    "\n",
    "#Compute new features\n",
    "print('Computing Feature Set')\n",
    "if kernel=='linear':\n",
    "    X_train_newFeatures = X_train\n",
    "elif kernel=='poly2':\n",
    "    poly = PolynomialFeatures(degree=2)\n",
    "    X_train_newFeatures=poly.fit_transform(X_train.toarray())\n",
    "elif kernel=='poly3':\n",
    "    poly = PolynomialFeatures(degree=3)\n",
    "    X_train_newFeatures=poly.fit_transform(X_train.toarray())\n",
    "elif kernel=='RBF':\n",
    "    rbf_feature = RBFSampler(gamma=1, random_state=1)#, n_components=25000)\n",
    "    X_train_newFeatures = rbf_feature.fit_transform(X_train)\n",
    "print('Feature Set Computed')\n",
    "\n",
    "#Train Model\n",
    "clf = SGDClassifier(loss=\"hinge\", penalty=\"l2\", max_iter=1e7, tol=1e-5, alpha=ALPHA)\n",
    "print('Fitting SVM')\n",
    "clf.fit(X_train_newFeatures,y_train)\n",
    "print('Fitting Done')\n",
    "\n",
    "#Predict\n",
    "y_model_train=clf.predict(X_train_newFeatures)\n",
    "misclassifiedFraction=(np.sum(np.fabs(y_model_train-y_train)))/(2*y_train.size)\n",
    "print('Misclassified Fraciton on Train Dataset='+str(misclassifiedFraction))\n",
    "\n",
    "confusion_matrix(y_train,y_model_train)\n",
    "\n",
    "#Save Results:\n",
    "model_filename = label\n",
    "\n",
    "joblib.dump(clf,label) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SGDClassifier(alpha=0.0001, average=False, class_weight=None, epsilon=0.1,\n",
      "       eta0=0.0, fit_intercept=True, l1_ratio=0.15,\n",
      "       learning_rate='optimal', loss='hinge', max_iter=10000000.0,\n",
      "       n_iter=None, n_jobs=1, penalty='l2', power_t=0.5, random_state=None,\n",
      "       shuffle=True, tol=1e-05, verbose=0, warm_start=False)\n",
      "Computing Feature Set\n",
      "Feature Set Computed\n",
      "Predicting\n",
      "(25000,)\n",
      "Misclassified Fraciton on Test Dataset=0.14144\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[10542,  1958],\n",
       "       [ 1578, 10922]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "###############################################################################################\n",
    "#                           Find Error on Testing Dataset                                     #\n",
    "#                Generate features appropriately based on model imported                      #\n",
    "###############################################################################################\n",
    "\n",
    "#Choose:\n",
    "#  File name for model testing\n",
    "#  Kernels from linear, polyD (polynomial with degree d), RBF based on model imported\n",
    "label=\"linear_with_bow.txt\"\n",
    "kernel='linear' \n",
    "\n",
    "# Load Model and Predict\n",
    "X_test=X[25010:,:]\n",
    "y_test=labels\n",
    "svm_model = joblib.load(label) \n",
    "print(svm_model)\n",
    "\n",
    "# Make appropriate features\n",
    "\n",
    "print('Computing Feature Set')\n",
    "if kernel=='linear':\n",
    "    X_test_newFeatures=X_test\n",
    "elif kernel=='poly2':\n",
    "    poly = PolynomialFeatures(degree=2)\n",
    "    X_test_newFeatures=poly.fit_transform(X_test.toarray())\n",
    "elif kernel=='poly3':\n",
    "    poly = PolynomialFeatures(degree=3)\n",
    "    X_test_newFeatures=poly.fit_transform(X_test.toarray())\n",
    "elif kernel=='RBF':\n",
    "    rbf_feature = RBFSampler(gamma=1, random_state=1)#, n_components=25000)\n",
    "    X_test_newFeatures = rbf_feature.fit_transform(X_test)\n",
    "print('Feature Set Computed')\n",
    "\n",
    "print('Predicting')\n",
    "y_model=svm_model.predict(X_test_newFeatures)\n",
    "print(y_model.shape)\n",
    "\n",
    "#print(np.unique(y_model))\n",
    "\n",
    "misclassifiedFraction=(np.sum(np.fabs(y_model-y_test)))/(2*y_test.size)\n",
    "print('Misclassified Fraciton on Test Dataset='+str(misclassifiedFraction))\n",
    "\n",
    "confusion_matrix(y_test,y_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SGDClassifier(alpha=0.0001, average=False, class_weight=None, epsilon=0.1,\n",
      "       eta0=0.0, fit_intercept=True, l1_ratio=0.15,\n",
      "       learning_rate='optimal', loss='hinge', max_iter=10000000.0,\n",
      "       n_iter=None, n_jobs=1, penalty='l2', power_t=0.5, random_state=None,\n",
      "       shuffle=True, tol=1e-05, verbose=0, warm_start=False)\n",
      "Computing Feature Set\n",
      "Feature Set Computed\n",
      "Predicting\n",
      "(50000,)\n",
      "Misclassified Fraciton=0.18836\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[10136,  2364],\n",
       "       [ 2345, 10155]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "###############################################################################################\n",
    "#                            Find Error on Adversarial Dataset                                #\n",
    "#                Generate features appropriately based on model imported                      #\n",
    "###############################################################################################\n",
    "\n",
    "#Choose:\n",
    "#  File name for model testing\n",
    "#  Kernels from linear, polyD (polynomial with degree d), RBF based on model imported\n",
    "label=\"linear_with_bow.txt\"\n",
    "kernel='linear' \n",
    "\n",
    "# Load Model and Predict\n",
    "X_test=X_adversary[50000:,:]        \n",
    "y_test=labels\n",
    "\n",
    "svm_model = joblib.load(label) \n",
    "print(svm_model)\n",
    "\n",
    "# Make appropriate features\n",
    "\n",
    "print('Computing Feature Set')\n",
    "if kernel=='linear':\n",
    "    X_test_newFeatures=X_test\n",
    "elif kernel=='poly2':\n",
    "    poly = PolynomialFeatures(degree=2)\n",
    "    X_test_newFeatures=poly.fit_transform(X_test.toarray())\n",
    "elif kernel=='poly3':\n",
    "    poly = PolynomialFeatures(degree=3)\n",
    "    X_test_newFeatures=poly.fit_transform(X_test.toarray())\n",
    "elif kernel=='RBF':\n",
    "    rbf_feature = RBFSampler(gamma=1, random_state=1)#, n_components=25000)\n",
    "    X_test_newFeatures = rbf_feature.fit_transform(X_test)\n",
    "print('Feature Set Computed')\n",
    "\n",
    "print('Predicting')\n",
    "y_model=svm_model.predict(X_test_newFeatures)\n",
    "print(y_model.shape)\n",
    "\n",
    "#print(np.unique(y_model))\n",
    "\n",
    "misclassifiedFraction=(np.sum(np.fabs(y_model[1::2]-y_test)))/(2*y_test.size)\n",
    "print('Misclassified Fraciton='+str(misclassifiedFraction))\n",
    "\n",
    "confusion_matrix(y_test,y_model[1::2])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
