{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading word list!\n",
      "Loaded the word list!\n",
      "Loading word vectors!\n",
      "Loaded the word vectors!\n",
      "400000\n",
      "(400000, 50)\n"
     ]
    }
   ],
   "source": [
    "###############################################################################################\n",
    "#                  Code for SVM Baseline using Word-to-Vec Features                           #\n",
    "#                        for Sentiment Analysis on IMDB dataset                               #\n",
    "###############################################################################################\n",
    "\n",
    "#Import Libraries\n",
    "\n",
    "#Import Matrix Handling Capabilities\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "#Import File Handling\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "import re\n",
    "#Import scikit library features for model manipulation\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.kernel_approximation import RBFSampler\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.externals import joblib\n",
    "\n",
    "#Add path to files containing movie review train dataset \n",
    "pwd='/Users/pulkit/Google Drive/Stan Courses/4. Fall 2017/CS229 ML/Project/CS229-Project/'\n",
    "datapath=pwd+'stanford_train/'\n",
    "\n",
    "#Import precomputed word list and word vectors\n",
    "print('Loading word list!')\n",
    "wordsList = np.load(datapath+'wordsList-lexic-sorted.npy').tolist()\n",
    "print('Loaded the word list!')\n",
    "print('Loading word vectors!')\n",
    "wordVectors = np.load(datapath+'wordVectors-lexic-sorted.npy')\n",
    "print ('Loaded the word vectors!')\n",
    "print(len(wordsList))\n",
    "print(wordVectors.shape)\n",
    "_,wordEncodingLen=wordVectors.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Going through Positive Files\n",
      "Positive files finished\n",
      "Going through Negative Files\n",
      "Negative files finished\n",
      "The total number of files is 25000\n",
      "The total number of words in the files is 5844680\n",
      "The average number of words in the files is 233.7872\n"
     ]
    }
   ],
   "source": [
    "# Find statistics of dataset to decide number of word to used for mki\n",
    "\n",
    "positiveFiles = [datapath+'pos/' + f for f in listdir(datapath+'pos/') if isfile(join(datapath+'pos/', f))]\n",
    "negativeFiles = [datapath+'neg/' + f for f in listdir(datapath+'neg/') if isfile(join(datapath+'neg/', f))]\n",
    "numWords = []\n",
    "\n",
    "print('Going through Positive Files')\n",
    "for pf in positiveFiles:\n",
    "    with open(pf, \"r\", encoding='utf-8') as f:\n",
    "        line=f.readline()\n",
    "        counter = len(line.split())\n",
    "        numWords.append(counter)       \n",
    "print('Positive files finished')\n",
    "\n",
    "print('Going through Negative Files')\n",
    "for nf in negativeFiles:\n",
    "    with open(nf, \"r\", encoding='utf-8') as f:\n",
    "        line=f.readline()\n",
    "        counter = len(line.split())\n",
    "        numWords.append(counter)  \n",
    "print('Negative files finished')\n",
    "\n",
    "numFiles = len(numWords)\n",
    "print('The total number of files is', numFiles)\n",
    "print('The total number of words in the files is', sum(numWords))\n",
    "print('The average number of words in the files is', sum(numWords)/len(numWords))\n",
    "\n",
    "#Define maximum sequence length to take into account in model based on previous statistics\n",
    "maxSeqLength=250"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Going through Positive Files for indices\n",
      "Positive files finished\n",
      "Going through Negative Files for indices\n",
      "Negative files finished\n",
      "(25000, 250)\n"
     ]
    }
   ],
   "source": [
    "##########################\n",
    "#  Import Training Data  #\n",
    "##########################\n",
    "\n",
    "#Go through files and find word IDs\n",
    "\n",
    "#define for string cleaning\n",
    "strip_special_chars = re.compile(\"[^A-Za-z0-9 ]+\")\n",
    "def cleanSentences(string):\n",
    "    '''\n",
    "    Cleans Sentences\n",
    "    '''\n",
    "    string = string.lower().replace(\"<br />\", \" \")\n",
    "    return re.sub(strip_special_chars, \"\", string.lower())\n",
    "\n",
    "#define for finding index in wordvec\n",
    "def findIndex(search_list, begin, end, key):\n",
    "    '''\n",
    "    Find Index of word in a list\n",
    "    '''\n",
    "    mid = int((end - begin + 1)/2) + begin\n",
    "    if end == begin:\n",
    "        if search_list[mid] == key:\n",
    "            return mid\n",
    "        else:\n",
    "            return -1\n",
    "    if end == begin + 1:\n",
    "        if search_list[begin] == key:\n",
    "            return begin\n",
    "        if search_list[end] == key:\n",
    "            return end\n",
    "        else:\n",
    "            return -1\n",
    "    if search_list[mid] < key:\n",
    "        return findIndex(search_list, mid, end, key)\n",
    "    return findIndex(search_list, begin, mid, key)\n",
    "\n",
    "ids = np.zeros((numFiles, maxSeqLength), dtype='int32')\n",
    "fileCounter = 0\n",
    "print('Going through Positive Files for indices')\n",
    "for pf in positiveFiles:\n",
    "    with open(pf, \"r\") as f:\n",
    "        indexCounter = 0\n",
    "        line=f.readline()\n",
    "        cleanedLine = cleanSentences(line)\n",
    "        split = cleanedLine.split()\n",
    "        for word in split:\n",
    "            try:\n",
    "                ids[fileCounter][indexCounter] = findIndex(wordsList, 0, len(wordsList)-1, word)\n",
    "            except ValueError:\n",
    "                ids[fileCounter][indexCounter] = 399999 #Vector for unknown words\n",
    "            indexCounter = indexCounter + 1\n",
    "            if indexCounter >= maxSeqLength:\n",
    "                break\n",
    "        fileCounter = fileCounter + 1 \n",
    "print('Positive files finished')\n",
    "\n",
    "print('Going through Negative Files for indices')\n",
    "for nf in negativeFiles:\n",
    "    with open(nf, \"r\") as f:\n",
    "        indexCounter = 0\n",
    "        line=f.readline()\n",
    "        cleanedLine = cleanSentences(line)\n",
    "        split = cleanedLine.split()\n",
    "        for word in split:\n",
    "            try:\n",
    "                ids[fileCounter][indexCounter] = findIndex(wordsList, 0, len(wordsList)-1, word)\n",
    "            except ValueError:\n",
    "                ids[fileCounter][indexCounter] = 399999 #Vector for unknown words\n",
    "            indexCounter = indexCounter + 1\n",
    "            if indexCounter >= maxSeqLength:\n",
    "                break\n",
    "        fileCounter = fileCounter + 1 \n",
    " #Pass into embedding function and see if it evaluates. \n",
    "print('Negative files finished')\n",
    "\n",
    "\n",
    "#ids=np.load(datapath+'idsMatrix.npy')\n",
    "print(ids.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Making Input Data Matrix\n",
      "Input Data Matrix Loaded\n",
      "(25000, 12500)\n"
     ]
    }
   ],
   "source": [
    "##########################\n",
    "#    Generate Features   #\n",
    "##########################\n",
    "\n",
    "labels=-1*np.ones(numFiles)\n",
    "labels[0:int(numFiles/2)]=np.ones(int(numFiles/2))\n",
    "\n",
    "#print(np.unique(labels))\n",
    "print('Making Input Data Matrix')\n",
    "\n",
    "#Choose feature type:\n",
    "#'allVectors': as all vectors implying each example is 12500 dimensional\n",
    "#'meanVectors': as all vectors implying each example is 50 dimensional\n",
    "\n",
    "featureType='allVectors'\n",
    "\n",
    "if featureType=='allVectors':\n",
    "    inputData=np.zeros((numFiles,maxSeqLength*wordEncodingLen))\n",
    "    for i in range(numFiles):\n",
    "            for j in range(maxSeqLength):\n",
    "                    inputData[i,j*wordEncodingLen:(j+1)*wordEncodingLen]=wordVectors[ids[i,j],]\n",
    "elif featureType=='meanVectors':\n",
    "    inputData=np.zeros((numFiles,wordEncodingLen))\n",
    "    for i in range(numFiles):\n",
    "        inputData[i,0:wordEncodingLen]=np.mean(wordVectors[ids[i,:],],axis=0)\n",
    "\n",
    "print('Input Data Matrix Loaded')\n",
    "\n",
    "print(inputData.shape)\n",
    "\n",
    "X_train=inputData\n",
    "y_train=labels\n",
    "\n",
    "#mean=np.mean(wordVectors[ids[i,:],],axis=0)\n",
    "#print(mean.shape)\n",
    "#print(wordVectors[ids[1,0:2],])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nfrom sklearn.model_selection import train_test_split\\n\\ndef splitData():\\n    print('Splitting Data')\\n    X_train, X_test, y_train, y_test = train_test_split(inputData[100:24900], labels[100:24900], test_size=0.1, random_state=42)\\n    print(X_train.shape,X_test.shape,y_train.shape,y_test.shape)\\n    #print(np.unique(y_train),np.unique(y_test))\\n    \\n    return X_train, X_test, y_train, y_test\\n    \\n#splitData()\\n\\nX_train, X_test, y_train, y_test=splitData()\\n\""
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# OLD FUNCTION FOR SPLITTING DATA\n",
    "# Left Middle 200 Reveiws for Validation:\n",
    "'''\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def splitData():\n",
    "    print('Splitting Data')\n",
    "    X_train, X_test, y_train, y_test = train_test_split(inputData[100:24900], labels[100:24900], test_size=0.1, random_state=42)\n",
    "    print(X_train.shape,X_test.shape,y_train.shape,y_test.shape)\n",
    "    #print(np.unique(y_train),np.unique(y_test))\n",
    "    \n",
    "    return X_train, X_test, y_train, y_test\n",
    "    \n",
    "#splitData()\n",
    "\n",
    "X_train, X_test, y_train, y_test=splitData()\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing Feature Set\n",
      "Feature Set Computed\n",
      "Fitting SVM\n",
      "Fitting Done\n",
      "Misclassified Fraciton on Train Set=0.11072\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['linear_with_n12500.txt']"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "###############################################################################################\n",
    "#                           Fit Models and check training error                               #\n",
    "#  Uses SGD with hinge loss and l2 normalization to fit SVM. Generate features appropriately  #\n",
    "###############################################################################################\n",
    "\n",
    "#Choose:\n",
    "#  Kernels from linear, polyD (polynomial with degree d), RBF\n",
    "#  L2 penalty al (start from 1e-4)\n",
    "#  File name for saving model\n",
    "kernel='linear' \n",
    "ALPHA=1e-4\n",
    "label=\"linear_with_n12500.txt\"\n",
    "\n",
    "#Compute new features\n",
    "print('Computing Feature Set')\n",
    "if kernel=='linear':\n",
    "    X_train_newFeatures = X_train\n",
    "elif kernel=='poly2':\n",
    "    poly = PolynomialFeatures(degree=2)\n",
    "    X_train_newFeatures=poly.fit_transform(X_train.toarray())\n",
    "elif kernel=='poly3':\n",
    "    poly = PolynomialFeatures(degree=3)\n",
    "    X_train_newFeatures=poly.fit_transform(X_train.toarray())\n",
    "elif kernel=='RBF':\n",
    "    rbf_feature = RBFSampler(gamma=1, random_state=1)#, n_components=25000)\n",
    "    X_train_newFeatures = rbf_feature.fit_transform(X_train)\n",
    "print('Feature Set Computed')\n",
    "\n",
    "#Train Model\n",
    "clf = SGDClassifier(loss=\"hinge\", penalty=\"l2\", max_iter=1e7, tol=1e-5, alpha=ALPHA)\n",
    "print('Fitting SVM')\n",
    "clf.fit(X_train_newFeatures,y_train)\n",
    "print('Fitting Done')\n",
    "\n",
    "y_model_train=clf.predict(X_train_newFeatures)\n",
    "misclassifiedFraction=(np.sum(np.fabs(y_model_train-y_train)))/(2*y_train.size)\n",
    "print('Misclassified Fraciton on Train Set='+str(misclassifiedFraction))\n",
    "confusion_matrix(y_train,y_model_train)\n",
    "\n",
    "#Save Model:\n",
    "model_filename = label\n",
    "joblib.dump(clf,label) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
